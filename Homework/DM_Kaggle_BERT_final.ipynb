{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scL14l3pkTqa",
        "outputId": "ddcf893f-412f-4e4a-de8b-33535a750446"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# %cd \"/content/drive/MyDrive\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-XLQzfdkVnw",
        "outputId": "8e3bca71-19aa-450d-f5e7-e6993e64bf8f"
      },
      "outputs": [],
      "source": [
        "# %cd \"/content/drive/MyDrive/DM-Kaggle\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lKBPNTMqiYdm"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/rita/anaconda3/envs/DM/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "labels = {'joy':0,\n",
        "      'anger':1,\n",
        "      'sadness':2,\n",
        "      'anticipation':3,\n",
        "      'disgust':4,\n",
        "      'trust':5,\n",
        "      'fear':6,\n",
        "      'surprise':7\n",
        "      }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qEPF4FVSjFV7"
      },
      "outputs": [],
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.labels = [labels[label] for label in df['emotion']]\n",
        "        self.texts = [tokenizer(text, padding='max_length', max_length = 128, truncation=True, return_tensors=\"pt\")\n",
        "                      for text in df['text']]\n",
        "\n",
        "    def classes(self):\n",
        "        return self.labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        # Fetch a batch of labels\n",
        "        return np.array(self.labels[idx])\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        # Fetch a batch of inputs\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "        return batch_texts, batch_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lsPG2duUkx9A"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mcoU_1Rcj32f"
      },
      "outputs": [],
      "source": [
        "train_data_path = './train_raw_data.csv'\n",
        "chunk_size = 1000\n",
        "chunks = []\n",
        "for chunk in pd.read_csv(train_data_path, chunksize=chunk_size, iterator=True):\n",
        "    chunks.append(chunk)\n",
        "train_df = pd.concat(chunks, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "R5NfKAkKnBwm"
      },
      "outputs": [],
      "source": [
        "train_df = train_df.dropna(axis=0, subset=['text', 'emotion'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IwUmEIXtkF1E"
      },
      "outputs": [],
      "source": [
        "test_data_path = './test_raw_data.csv'\n",
        "chunk_size = 1000\n",
        "chunks = []\n",
        "for chunk in pd.read_csv(test_data_path, chunksize=chunk_size, iterator=True):\n",
        "    chunks.append(chunk)\n",
        "test_df = pd.concat(chunks, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "786NolnPjhFY",
        "outputId": "7bd7e8cb-0cef-4c1b-e25b-b1194cf1b68d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/rita/anaconda3/envs/DM/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1161422 290356 414817\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(112)\n",
        "df_train, df_val = np.split(train_df.sample(frac=1, random_state=42), [int(.8*len(train_df))])\n",
        "df_test = test_df\n",
        "\n",
        "print(len(df_train),len(df_val), len(df_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tULomcrClGDx"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from transformers import BertModel\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "    def __init__(self, dropout=0.5):\n",
        "        super(BertClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, 8)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input_id, mask):\n",
        "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        final_layer = self.relu(linear_output)\n",
        "        return final_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "__ue0MDzlNIs"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "model_dir = './checkpoints'\n",
        "def train(model, train_data, val_data, learning_rate, epochs):\n",
        "  # 通过Dataset类获取训练和验证集\n",
        "    train, val = Dataset(train_data), Dataset(val_data)\n",
        "    # DataLoader根据batch_size获取数据，训练时选择打乱样本\n",
        "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=64, shuffle=True)\n",
        "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=64)\n",
        "  # 判断是否使用GPU\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    # 定义损失函数和优化器\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    if use_cuda:\n",
        "        model = model.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "    # 开始进入训练循环\n",
        "        min_loss = 999\n",
        "    for epoch_num in range(epochs):\n",
        "      # 定义两个变量，用于存储训练集的准确率和损失\n",
        "        total_acc_train = 0\n",
        "        total_loss_train = 0\n",
        "      # 进度条函数tqdm\n",
        "        for train_input, train_label in tqdm(train_dataloader):\n",
        "\n",
        "                train_label = train_label.to(device)\n",
        "                mask = train_input['attention_mask'].to(device)\n",
        "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "        # 通过模型得到输出\n",
        "                output = model(input_id, mask)\n",
        "                # 计算损失\n",
        "                batch_loss = criterion(output, train_label)\n",
        "                total_loss_train += batch_loss.item()\n",
        "                # 计算精度\n",
        "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
        "                total_acc_train += acc\n",
        "        # 模型更新\n",
        "                model.zero_grad()\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "        if batch_loss < min_loss:\n",
        "                min_loss = batch_loss\n",
        "                torch.save(model.state_dict(), os.path.join(model_dir , 'GPT_best.pt'))\n",
        "                torch.save(model, os.path.join(model_dir , 'GPT_best_model.pt'))\n",
        "            # ------ 验证模型 -----------\n",
        "            # 定义两个变量，用于存储验证集的准确率和损失\n",
        "        total_acc_val = 0\n",
        "        total_loss_val = 0\n",
        "      # 不需要计算梯度\n",
        "        with torch.no_grad():\n",
        "                # 循环获取数据集，并用训练好的模型进行验证\n",
        "                for val_input, val_label in val_dataloader:\n",
        "          # 如果有GPU，则使用GPU，接下来的操作同训练\n",
        "                    val_label = val_label.to(device)\n",
        "                    mask = val_input['attention_mask'].to(device)\n",
        "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                    output = model(input_id, mask)\n",
        "\n",
        "                    batch_loss = criterion(output, val_label)\n",
        "                    total_loss_val += batch_loss.item()\n",
        "\n",
        "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
        "                    total_acc_val += acc\n",
        "\n",
        "        print(\n",
        "                f'''Epochs: {epoch_num + 1}\n",
        "              | Train Loss: {total_loss_train / len(train_data): .3f}\n",
        "              | Train Accuracy: {total_acc_train / len(train_data): .3f}\n",
        "              | Val Loss: {total_loss_val / len(val_data): .3f}\n",
        "              | Val Accuracy: {total_acc_val / len(val_data): .3f}''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hExBHloTliZf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 18148/18148 [1:23:40<00:00,  3.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 1\n",
            "              | Train Loss:  0.021\n",
            "              | Train Accuracy:  0.517\n",
            "              | Val Loss:  0.019\n",
            "              | Val Accuracy:  0.578\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 18148/18148 [1:23:34<00:00,  3.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 2\n",
            "              | Train Loss:  0.018\n",
            "              | Train Accuracy:  0.597\n",
            "              | Val Loss:  0.017\n",
            "              | Val Accuracy:  0.605\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 18148/18148 [1:24:13<00:00,  3.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 3\n",
            "              | Train Loss:  0.017\n",
            "              | Train Accuracy:  0.621\n",
            "              | Val Loss:  0.017\n",
            "              | Val Accuracy:  0.617\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 18148/18148 [1:24:52<00:00,  3.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 4\n",
            "              | Train Loss:  0.016\n",
            "              | Train Accuracy:  0.637\n",
            "              | Val Loss:  0.016\n",
            "              | Val Accuracy:  0.625\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 18148/18148 [1:25:52<00:00,  3.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 5\n",
            "              | Train Loss:  0.015\n",
            "              | Train Accuracy:  0.651\n",
            "              | Val Loss:  0.016\n",
            "              | Val Accuracy:  0.630\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 5\n",
        "model = BertClassifier()\n",
        "LR = 1e-6\n",
        "train(model, df_train, df_val, LR, EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), os.path.join(model_dir , 'BERT_final.pt'))\n",
        "torch.save(model, os.path.join(model_dir , 'BERT_final_model.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_save = model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = torch.load(\"./checkpoints/BERT_final_model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df):\n",
        "        # self.labels = [labels[label] for label in df['emotion']]\n",
        "        # self.ids = df['tweet_id']\n",
        "        self.texts = [tokenizer(text, padding='max_length', max_length = 128, truncation=True, return_tensors=\"pt\")\n",
        "                      for text in df['text']]\n",
        "\n",
        "    # def classes(self):\n",
        "    #     return self.texts\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    # def get_batch_ids(self, idx):\n",
        "    #     # Fetch a batch of labels\n",
        "    #     return np.array(self.ids[idx])\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        # Fetch a batch of inputs\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        # batch_ids = self.get_batch_ids(idx)\n",
        "        return batch_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 'joy', 1: 'anger', 2: 'sadness', 3: 'anticipation', 4: 'disgust', 5: 'trust', 6: 'fear', 7: 'surprise'}\n"
          ]
        }
      ],
      "source": [
        "reverse_labels = {value: key for key, value in labels.items()}\n",
        "print(reverse_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def evaluate(model, test_data):\n",
        "#     test = TestDataset(test_data)\n",
        "#     test_dataloader = torch.utils.data.DataLoader(test, batch_size=1)\n",
        "#     use_cuda = torch.cuda.is_available()\n",
        "#     device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "#     if use_cuda:\n",
        "#         model = model.cuda()\n",
        "\n",
        "#     total_acc_test = 0\n",
        "#     with torch.no_grad():\n",
        "#         for test_input in tqdm(test_dataloader):\n",
        "#             mask = test_input['attention_mask'].to(device)\n",
        "#             input_id = test_input['input_ids'].squeeze(1).to(device)\n",
        "#             output = model(input_id, mask)\n",
        "#             emotion_encoding = output.argmax(dim=1)\n",
        "#             emotion = reverse_labels.get(emotion_encoding)\n",
        "#             # print(test_id)\n",
        "#             print(test_input)\n",
        "#             print(emotion)\n",
        "#             break\n",
        "            \n",
        "    \n",
        "# evaluate(model, test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import trange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, test_data):\n",
        "    # test = TestDataset(test_data)\n",
        "    # test_dataloader = torch.utils.data.DataLoader(test, batch_size=1)\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    if use_cuda:\n",
        "        model = model.cuda()\n",
        "\n",
        "    prediction_list = []\n",
        "    with torch.no_grad():\n",
        "        for index in trange(len(test_data)):\n",
        "            try:\n",
        "                test_input = tokenizer(test_data['text'][index], padding='max_length', max_length = 128, truncation=True, return_tensors=\"pt\")\n",
        "                mask = test_input['attention_mask'].to(device)\n",
        "                input_id = test_input['input_ids'].squeeze(1).to(device)\n",
        "                output = model(input_id, mask)\n",
        "                emotion_encoding = output.argmax(dim=1)\n",
        "                emotion = reverse_labels.get(emotion_encoding.cpu().item())\n",
        "            except: \n",
        "                emotion = 'joy'\n",
        "            dict = {'id': test_data['tweet_id'][index], 'emotion': emotion}\n",
        "            prediction_list.append(dict)\n",
        "            # print(test_id)\n",
        "    prediction = pd.DataFrame(prediction_list)\n",
        "    return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Confident of your obedience, I write to you, knowing that you will do even more than I ask. (Philemon 1:21) 3/4 #bibleverse <LH> <LH>'"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df['text'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 414817/414817 [19:03<00:00, 362.88it/s] \n"
          ]
        }
      ],
      "source": [
        "prediction = evaluate(model, test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "414817\n"
          ]
        }
      ],
      "source": [
        "print(len(test_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         id       emotion\n",
            "0  0x28b412  anticipation\n",
            "1  0x2de201         trust\n",
            "2  0x218443           joy\n",
            "3  0x2939d5  anticipation\n",
            "4  0x26289a         trust\n"
          ]
        }
      ],
      "source": [
        "print(prediction.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "prediction_ver1 = prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "411972\n"
          ]
        }
      ],
      "source": [
        "print(len(prediction_ver1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "prediction_ver1 = prediction_ver1.drop_duplicates(subset=['id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "prediction_ver1 = prediction_ver1.dropna(subset=['id'])\n",
        "prediction_ver1 = prediction_ver1.dropna(subset=['emotion'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "414817\n"
          ]
        }
      ],
      "source": [
        "print(len(prediction))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "prediction_ver1.to_csv('submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install -q kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████| 6.19M/6.19M [00:03<00:00, 2.01MB/s]\n",
            "Successfully submitted to DM2023 ISA5810 Lab2 Homework"
          ]
        }
      ],
      "source": [
        "! kaggle competitions submit -c dm2023-isa5810-lab2-homework -f submission.csv -m \"BERT\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
